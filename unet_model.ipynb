{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Conv2DTranspose, Concatenate, Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "lr = 1e-4\n",
    "epochs = 100\n",
    "height = 1450\n",
    "width = 1340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"dataset\",\"non-aug\")\n",
    "files_dir = os.path.join(\"files\",\"non-aug\")\n",
    "\n",
    "model_file = os.path.join(files_dir,\"unet-non-aug.h5\")\n",
    "log_file = os.path.join(files_dir, \"log-non-aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir(files_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Building Unet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters,3,padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = MaxPooling2D(2, 2)(x)\n",
    "    return x,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(inputs, skip, num_filters):\n",
    "    x = Conv2DTranspose(num_filters,(2,2),strides=2,padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip])\n",
    "    x = conv_block(x, num_filters)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2,256)\n",
    "    s4, p4 = encoder_block(p3,512)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "    model = Model(inputs,outputs, name = \"UNET\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train_x = sorted(glob(os.path.join(path, \"train\", \"images\", \"*\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"train\", \"mask\", \"*\")))\n",
    "\n",
    "    valid_x = sorted(glob(os.path.join(path, \"valid\", \"images\", \"*\")))\n",
    "    valid_y = sorted(glob(os.path.join(path, \"valid\", \"mask\", \"*\")))\n",
    "\n",
    "    return (train_x,train_y), (valid_x,valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = x / 255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readin Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = x / 250.0\n",
    "    x = np.expand_dims(x, axis= -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x,y\n",
    "    \n",
    "    x,y = tf.numpy_function(_parse,[x,y], [tf.float64, tf.float64])\n",
    "    x.set_shape([height, width, 3])\n",
    "    y.set_shape([height, width, 1])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(x, y , batch = 8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15 - 15\n",
      "Valid: 5 - 5\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y),(valid_x, valid_y) = load_data(dataset_path)\n",
    "print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 180, 166, 512), (None, 181, 167, 512)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (height, width, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m build_unet(input_shape)\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mbuild_unet\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     11\u001b[0m b1 \u001b[38;5;241m=\u001b[39m conv_block(p4, \u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Decoder \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m d1 \u001b[38;5;241m=\u001b[39m decoder_block(b1, s4, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     15\u001b[0m d2 \u001b[38;5;241m=\u001b[39m decoder_block(d1, s3, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     16\u001b[0m d3 \u001b[38;5;241m=\u001b[39m decoder_block(d2, s2, \u001b[38;5;241m128\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mdecoder_block\u001b[0;34m(inputs, skip, num_filters)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecoder_block\u001b[39m(inputs, skip, num_filters):\n\u001b[1;32m      2\u001b[0m     x \u001b[38;5;241m=\u001b[39m Conv2DTranspose(num_filters,(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(inputs)\n\u001b[0;32m----> 3\u001b[0m     x \u001b[38;5;241m=\u001b[39m Concatenate()([x, skip])\n\u001b[1;32m      4\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv_block(x, num_filters)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/merging/concatenate.py:97\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     91\u001b[0m         unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m     92\u001b[0m             shape[axis]\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 180, 166, 512), (None, 181, 167, 512)]"
     ]
    }
   ],
   "source": [
    "input_shape = (height, width, 3)\n",
    "model = build_unet(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.Optimizer.Adam(lr)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(model_file, verbose= 1, save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0,1,patience=4),\n",
    "    CSVLogger(log_file),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = valid_dataset,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
